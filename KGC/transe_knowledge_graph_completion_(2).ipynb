{
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "do2yT94Yagds",
        "HM0ax6rPpbpe",
        "ZTE3qGJ-qPcX",
        "1xP_BA1Gmto3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install AmpliGraph and other dependencies"
      ],
      "metadata": {
        "id": "UM6Awy5WFUVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ampligraph is a Library for Representation Learning on Knowledge Graphs. For Discovering new knowledge from an existing knowledge graph, Complete large knowledge graphs with missing statements, Generate stand-alone knowledge graph embeddings and evaluate a new relational model are some of the usecases of Ampligraph module."
      ],
      "metadata": {
        "id": "bLsTcMqfZBMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.0\n",
        "import tensorflow as tf \n",
        "\n",
        "print('TensorFlow  version: {}'.format(tf.__version__))"
      ],
      "metadata": {
        "id": "vs4xn9CWE5Yx",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:03:54.328787Z",
          "iopub.execute_input": "2022-12-26T05:03:54.329648Z",
          "iopub.status.idle": "2022-12-26T05:05:25.802091Z",
          "shell.execute_reply.started": "2022-12-26T05:03:54.329498Z",
          "shell.execute_reply": "2022-12-26T05:05:25.800585Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "# Install AmpliGraph library\n",
        "! pip install ampligraph\n",
        "\n",
        "# Required to visualize embeddings with tensorboard projector, comment out if not required!\n",
        "! pip install --user tensorboard\n",
        "\n",
        "# Required to plot text on embedding clusters, comment out if not required!\n",
        "! pip install --user git+https://github.com/Phlya/adjustText"
      ],
      "metadata": {
        "id": "Lgs8cTcu9hUM",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:05:25.804771Z",
          "iopub.execute_input": "2022-12-26T05:05:25.805310Z",
          "iopub.status.idle": "2022-12-26T05:06:27.475617Z",
          "shell.execute_reply.started": "2022-12-26T05:05:25.805254Z",
          "shell.execute_reply": "2022-12-26T05:06:27.473775Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports used in this tutorial \n",
        "# %tensorflow_version 1.15\n",
        "import ampligraph\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from ampligraph.datasets import load_fb15k_237\n",
        "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score\n",
        "from ampligraph.discovery import query_topn, discover_facts, find_clusters\n",
        "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "\n",
        "def display_aggregate_metrics(ranks):\n",
        "    print('Mean Rank:', mr_score(ranks)) \n",
        "    print('Mean Reciprocal Rank:', mrr_score(ranks)) \n",
        "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
        "    print('Hits@10:', hits_at_n_score(ranks, 10))\n",
        "    print('Hits@100:', hits_at_n_score(ranks, 100))\n",
        "\n",
        "print('Ampligraph version: {}'.format(ampligraph.__version__))"
      ],
      "metadata": {
        "id": "KqGjJ_SYFxIH",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:27.477616Z",
          "iopub.execute_input": "2022-12-26T05:06:27.478033Z",
          "iopub.status.idle": "2022-12-26T05:06:29.501622Z",
          "shell.execute_reply.started": "2022-12-26T05:06:27.477991Z",
          "shell.execute_reply": "2022-12-26T05:06:29.500309Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading a Knowledge Graph dataset\n",
        "\n",
        "To begin with we're going to need a knowledge graph, so let's load a standard knowledge graph called ***Freebase-15k-237***. Used APIs from Ampligraph to load the freebase-15k-237 dataset.\n"
      ],
      "metadata": {
        "id": "E2zQO7QqjYQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.datasets import load_fb15k_237"
      ],
      "metadata": {
        "id": "IYLn3NXKegOm",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:29.504596Z",
          "iopub.execute_input": "2022-12-26T05:06:29.505003Z",
          "iopub.status.idle": "2022-12-26T05:06:29.511453Z",
          "shell.execute_reply.started": "2022-12-26T05:06:29.504962Z",
          "shell.execute_reply": "2022-12-26T05:06:29.509706Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For this tutorial we have remapped the IDs of freebase 237 and created a csv file containing human readable names instead of IDs. Following is a sample of the dataset."
      ],
      "metadata": {
        "id": "MKrD7K04egPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "URL = 'https://ampgraphenc.s3-eu-west-1.amazonaws.com/datasets/freebase-237-merged-and-remapped.csv'\n",
        "dataset = pd.read_csv(URL, header=None)\n",
        "dataset.columns = ['subject', 'predicate', 'object']\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "id": "cHzhvBhbegPX",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:29.513509Z",
          "iopub.execute_input": "2022-12-26T05:06:29.513925Z",
          "iopub.status.idle": "2022-12-26T05:06:35.459795Z",
          "shell.execute_reply.started": "2022-12-26T05:06:29.513878Z",
          "shell.execute_reply": "2022-12-26T05:06:35.458272Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total triples in the KG:', dataset.shape)"
      ],
      "metadata": {
        "id": "4pgpcidHegQC",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:35.461784Z",
          "iopub.execute_input": "2022-12-26T05:06:35.462324Z",
          "iopub.status.idle": "2022-12-26T05:06:35.470618Z",
          "shell.execute_reply.started": "2022-12-26T05:06:35.462276Z",
          "shell.execute_reply": "2022-12-26T05:06:35.468910Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![KG](https://user-images.githubusercontent.com/39597669/90747195-9fc44c80-e2c8-11ea-9f70-097993581bac.png) \n"
      ],
      "metadata": {
        "id": "flR0xOXmegP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.1 Create training, validation and test splits\n",
        "\n",
        "Let's use the [`train_test_split_no_unseen`](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.train_test_split_no_unseen.html?#train-test-split-no-unseen) function provided by Ampligraph to create the training, validation and test splits. \n",
        "\n",
        "This API ensures that the test and validation splits contains triples whose entities are \"seen\" during training. \n"
      ],
      "metadata": {
        "id": "PQJkziMCegQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import train_test_split_no_unseen\n",
        "# get the validation set of size 500\n",
        "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed=0)\n",
        "\n",
        "# get the test set of size 1000 from the remaining triples\n",
        "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed=0)\n",
        "\n",
        "print('Total triples:', dataset.shape)\n",
        "print('Size of train:', X_train.shape)\n",
        "print('Size of valid:', X_valid.shape)\n",
        "print('Size of test:', X_test.shape)"
      ],
      "metadata": {
        "id": "ltijAdQtegQN",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:35.472470Z",
          "iopub.execute_input": "2022-12-26T05:06:35.473470Z",
          "iopub.status.idle": "2022-12-26T05:06:37.664683Z",
          "shell.execute_reply.started": "2022-12-26T05:06:35.473418Z",
          "shell.execute_reply": "2022-12-26T05:06:37.663391Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train_test_split_no_unseen` API can be used to generate train/test splits such that test set contains only entities 'seen' during training"
      ],
      "metadata": {
        "id": "4Q38cXnYHHI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Training\n",
        "Now that we have split the dataset, let's dive directly into model training. \n",
        "\n",
        "Let us create a TransE model and train it on the training split using the `fit` function.\n",
        "\n",
        "**TransE** is one of the first embedding models which set the platform for the KGE research. It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. \n"
      ],
      "metadata": {
        "id": "fgAcQ1g3egQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import TransE\n",
        "\n",
        "model = TransE(k=150,                                                             # embedding size\n",
        "               epochs=100,                                                        # Num of epochs\n",
        "               batches_count= 10,                                                 # Number of batches \n",
        "               eta=1,                                                             # number of corruptions to generate during training\n",
        "               loss='pairwise', loss_params={'margin': 1},                        # loss type and it's hyperparameters         \n",
        "               initializer='xavier', initializer_params={'uniform': False},       # initializer type and it's hyperparameters\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},   # regularizer along with its hyperparameters\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001},                # optimizer to use along with its hyperparameters\n",
        "               seed= 0, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "save_model(model, 'TransE-small.pkl')"
      ],
      "metadata": {
        "id": "ap1Yd4LEegQg",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:06:37.666402Z",
          "iopub.execute_input": "2022-12-26T05:06:37.666834Z",
          "iopub.status.idle": "2022-12-26T05:07:49.125048Z",
          "shell.execute_reply.started": "2022-12-26T05:06:37.666798Z",
          "shell.execute_reply": "2022-12-26T05:07:49.123570Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Compute the evaluation metrics\n",
        "\n",
        "### Per triple metrics:\n",
        "This is a metric that is computed for each test set triple:\n",
        "\n",
        "- **score**: This is the value assigned to a triple, by the model, by applying the scoring function.\n",
        "\n",
        "Let's look at how we can get the score for a triple of interest and how to interpret it.\n"
      ],
      "metadata": {
        "id": "JaJeVr-megQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_triple = ['harrison ford', \n",
        "               '/film/actor/film./film/performance/film', \n",
        "               'star wars']\n",
        "\n",
        "triple_score = model.predict(test_triple)\n",
        "\n",
        "print('Triple of interest:\\n', test_triple)\n",
        "print('Triple Score:\\n', triple_score)"
      ],
      "metadata": {
        "id": "edjJcTReegQs",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.127155Z",
          "iopub.execute_input": "2022-12-26T05:07:49.127609Z",
          "iopub.status.idle": "2022-12-26T05:07:49.248122Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.127571Z",
          "shell.execute_reply": "2022-12-26T05:07:49.247047Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what does this score tell you? Nothing! It is just a value. In order to interpret the score we have 2 options:\n",
        "\n",
        "1. We can create a list of hypothesis that we want to test, score them and then choose the top n hypothesis as True statements.\n",
        "\n",
        "2. As described earlier in the theory section, unlike classification task, we are doing a learning to rank task. In order to interpret the score we can generate the corruptions and compare the triple score against the scores of corruptions to see how well does the model rank the test triple against them.\n",
        "\n",
        "\n",
        "Let's look at the first option. Let us create a list of hypothesis and score them."
      ],
      "metadata": {
        "id": "mQwswE4qFArO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "list_of_actors = ['salma hayek', 'carrie fisher', 'natalie portman',  'kristen bell',\n",
        "                  'mark hamill', 'neil patrick harris', 'harrison ford' ]\n",
        "\n",
        "# stack it horizontally to create s, p, o\n",
        "hypothesis = np.column_stack([list_of_actors, \n",
        "                              ['/film/actor/film./film/performance/film'] * len(list_of_actors),\n",
        "                              ['star wars'] * len(list_of_actors),\n",
        "                             ])\n",
        "\n",
        "# score the hypothesis\n",
        "triple_scores = model.predict(hypothesis)\n",
        "\n",
        "# append the scores column\n",
        "scored_hypothesis = np.column_stack([hypothesis, triple_scores])\n",
        "# sort by score in descending order\n",
        "scored_hypothesis = scored_hypothesis[np.argsort(scored_hypothesis[:, 3])]\n",
        "scored_hypothesis"
      ],
      "metadata": {
        "id": "zKewmQmp-1od",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.252431Z",
          "iopub.execute_input": "2022-12-26T05:07:49.253103Z",
          "iopub.status.idle": "2022-12-26T05:07:49.383872Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.253066Z",
          "shell.execute_reply": "2022-12-26T05:07:49.382345Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **rank**: For a triple, this metric is computed by generating corruptions and then scoring them and computing the rank(position) of the triple score against the corruptions. The pseudocode and the example illustrates how to compute rank on the test set.\n",
        "\n",
        "         for each test set triple <s, p, o>:\n",
        "                 a. Compute the score of the test triple (hypothesis) \n",
        "                     hypothesis_score = score(<s, p, o>)\n",
        "                     \n",
        "                 b. Generate the subject corruptions \n",
        "                         sub_corr = <?, p, o>\n",
        "                 c. Compute the score of the subject corruptions\n",
        "                         sub_corr_score = score(sub_corr) \n",
        "                 d. Find the position of hypothesis_score in sub_corr_score to get the sub_rank\n",
        "                   \n",
        "                 e. Generate the object corruption \n",
        "                         obj_corr = <s, p, ?>\n",
        "                 f. Compute the score of the object corruptions\n",
        "                         obj_corr_score = score(obj_corr) \n",
        "                 g. Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
        "                 \n",
        "                 h. Return rank = [sub_rank, obj_rank]\n",
        "\n",
        "\n",
        "\n",
        "![rank example](https://user-images.githubusercontent.com/281477/90627614-14897f00-e214-11ea-8f8e-d57da9888606.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4UUNCR-p_EmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Illustrative Example "
      ],
      "metadata": {
        "id": "do2yT94Yagds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the score of the test triple**"
      ],
      "metadata": {
        "id": "dImehz68LHQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_triple = ['harrison ford', \n",
        "               '/film/actor/film./film/performance/film', \n",
        "               'star wars']\n",
        "\n",
        "triple_score = model.predict(test_triple)\n",
        "\n",
        "print('Triple of interest:\\n', test_triple)\n",
        "print('Triple Score:\\n', triple_score)"
      ],
      "metadata": {
        "id": "dpRVBVn_K-_S",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.385483Z",
          "iopub.execute_input": "2022-12-26T05:07:49.385903Z",
          "iopub.status.idle": "2022-12-26T05:07:49.507704Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.385854Z",
          "shell.execute_reply": "2022-12-26T05:07:49.506245Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating the corruptions, let us look at the number of unique entities present in our dataset"
      ],
      "metadata": {
        "id": "SlLprqLE_Hby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The number of unique entities:', len(model.ent_to_idx))"
      ],
      "metadata": {
        "id": "PMEBkOW1VoOS",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.508889Z",
          "iopub.execute_input": "2022-12-26T05:07:49.510306Z",
          "iopub.status.idle": "2022-12-26T05:07:49.517160Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.510258Z",
          "shell.execute_reply": "2022-12-26T05:07:49.515300Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate the subject *corruptions* and compute rank**\n",
        "> ```sub_corr = <?, p, o>```"
      ],
      "metadata": {
        "id": "UXBikfbWkuK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subj_corr =  np.column_stack([list(model.ent_to_idx.keys()),\n",
        "                [test_triple[1]] * len(model.ent_to_idx), \n",
        "                [test_triple[2]] * len(model.ent_to_idx)])\n",
        "\n",
        "print('Subject corruptions:\\n', subj_corr)\n",
        "print('\\nSize of subject corruptions:\\n', subj_corr.shape)"
      ],
      "metadata": {
        "id": "WE6lw5FB_I_4",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.519113Z",
          "iopub.execute_input": "2022-12-26T05:07:49.520002Z",
          "iopub.status.idle": "2022-12-26T05:07:49.553780Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.519938Z",
          "shell.execute_reply": "2022-12-26T05:07:49.552316Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the score of the subject corruptions**"
      ],
      "metadata": {
        "id": "vO1HXo4x7t2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_corr_score = model.predict(subj_corr)\n",
        "sub_corr_score"
      ],
      "metadata": {
        "id": "N65INkWR_LFx",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.555732Z",
          "iopub.execute_input": "2022-12-26T05:07:49.556290Z",
          "iopub.status.idle": "2022-12-26T05:07:49.708027Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.556219Z",
          "shell.execute_reply": "2022-12-26T05:07:49.706516Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a score, let us compute the rank as follows:\n",
        "\n",
        "<center>$COUNT ( corruption_{score} >= triple_{score} )$</center>\n",
        "\n",
        "Find the position of hypothesis_score in sub_corr_score to get the sub_rank"
      ],
      "metadata": {
        "id": "Vu7vlKdg_OoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_rank_worst = np.sum(np.greater_equal(sub_corr_score, triple_score[0])) + 1\n",
        "\n",
        "print('Assigning the worst rank (to break ties):', sub_rank_worst)"
      ],
      "metadata": {
        "id": "V070u2N2_NBM",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.709943Z",
          "iopub.execute_input": "2022-12-26T05:07:49.710473Z",
          "iopub.status.idle": "2022-12-26T05:07:49.718320Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.710426Z",
          "shell.execute_reply": "2022-12-26T05:07:49.716917Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate the object *corruptions* and compute rank**\n",
        "\n",
        ">    ``` obj_corr = <s, p, ?> ```\n"
      ],
      "metadata": {
        "id": "ADOTNTGakqAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_corr =  np.column_stack([\n",
        "                [test_triple[0]] * len(model.ent_to_idx),\n",
        "                [test_triple[1]] * len(model.ent_to_idx), \n",
        "                     list(model.ent_to_idx.keys())])\n",
        "\n",
        "\n",
        "print('Object corruptions:\\n', obj_corr)\n",
        "print('\\nSize of object corruptions:\\n', obj_corr.shape)\n",
        "\n",
        "# f. Compute the score of the object corruptions\n",
        "obj_corr_score = model.predict(obj_corr)\n",
        "\n",
        "# g. Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
        "obj_rank_worst = np.sum(np.less_equal(triple_score[0], obj_corr_score)) + 1\n",
        "print('Assigning the worst rank (to break ties):', obj_rank_worst)\n"
      ],
      "metadata": {
        "id": "1MBRrCM0_RRB",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.719979Z",
          "iopub.execute_input": "2022-12-26T05:07:49.720502Z",
          "iopub.status.idle": "2022-12-26T05:07:49.895521Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.720466Z",
          "shell.execute_reply": "2022-12-26T05:07:49.893734Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Subject corruption rank:', sub_rank_worst)\n",
        "print('Object corruption rank:', obj_rank_worst)"
      ],
      "metadata": {
        "id": "F2OPFtvC_TJL",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.898541Z",
          "iopub.execute_input": "2022-12-26T05:07:49.899240Z",
          "iopub.status.idle": "2022-12-26T05:07:49.904960Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.899184Z",
          "shell.execute_reply": "2022-12-26T05:07:49.903806Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing the (Unfiltered) rank using evaluate_performance API**\n",
        "\n",
        "We can use the [evaluate_performance](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.evaluate_performance.html) API to compute the ranks. By default, `evaluate_performance` API computes the unfiltered ranks i.e. if any true positives are present in corruptions, they will not be removed before ranking. However, usually for evaluation, we follow a filtered evaluation as described in the next section.\n"
      ],
      "metadata": {
        "id": "V-d0JAxjC6ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import evaluate_performance \n",
        "\n",
        "ranks = evaluate_performance(np.array([test_triple]), \n",
        "                             model=model,\n",
        "                             ranking_strategy='worst')\n",
        "\n",
        "print('\\nRanks:', ranks)"
      ],
      "metadata": {
        "id": "06VnAcRyC5gk",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:49.906129Z",
          "iopub.execute_input": "2022-12-26T05:07:49.907221Z",
          "iopub.status.idle": "2022-12-26T05:07:50.349186Z",
          "shell.execute_reply.started": "2022-12-26T05:07:49.907181Z",
          "shell.execute_reply": "2022-12-26T05:07:50.347753Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple strategies to compute ranks especially when there are ties. Lets look at each of them in detail with an example. \n",
        "\n",
        "Assume there are only 10 corruptions, and assume that all the corruptions get the same score as the test triple. The ranks are as follows \n",
        "- Assign the **worst rank** i.e. the test set triple gets a rank of 11. This is followed by most papers in the literature. This is the strictest approach and it drives down the mrr by a large margin if there are many ties. We employ this strategy in AmpliGraph.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\ge hypothesis_{score} )$ + 1</center>\n",
        "    \n",
        "- Assign the **middle rank** i.e. the test set triple gets a rank of 6. We found this strategy being used by [ICLR 2020 paper](https://openreview.net/pdf?id=BkxSmlBFvr). This approach seems to be fair towards the model in resolving the ties as it assigns the middle rank to break ties.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\gt hypothesis_{score} ) + \\dfrac{COUNT( corruption_{score} == hypothesis_{score} )}{2}$ + 1</center>\n",
        "\n",
        "- Assign the **best rank** i.e. the test set triple gets a rank of 1. This approach is followed by [ConvKB paper](https://arxiv.org/pdf/1712.02121.pdf).  This approach is overly biased and helps the model achieve a very good mrr in case of ties.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\gt hypothesis_{score} )$ + 1</center>\n",
        "\n",
        "We recommend the usage of the **worst** strategy (default)."
      ],
      "metadata": {
        "id": "KgkpA_BWk_uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Filtered evaluation\n",
        "While evaluating ([as described earlier](#Compute-the-evaluation-metrics)), we generate all the corruptions (using all the unique entities in our dataset) per test triple, score and rank them. While doing so, we are not filtering the true positives - in other words, some of the corruptions may not really be corruptions and may be ground truth triples observed during training. Training triples usually get a high score as they are \"observed\" by the model. Hence a test triple would get a lower rank if such triples appear in corruptions. To filter out the True Positives (after step b. and e.), one can pass all the True Positive triples  to `filter_triples` parameter of the `evaluate_performance` API. This will perform a **\"filtered\" evaluation** and return the **\"filtered\" ranks** adjusted by removing the True Positives from the corruptions. More details for `evaluate_performance` API can be found [here](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance).\n"
      ],
      "metadata": {
        "id": "PuJpqTDklDux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import evaluate_performance \n",
        "\n",
        "print('Size of X_test:', X_test.shape)\n",
        "\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "ranks = evaluate_performance(np.array([test_triple]), \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "print(ranks)"
      ],
      "metadata": {
        "id": "YLD9MgxkC5Oo",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:50.350975Z",
          "iopub.execute_input": "2022-12-26T05:07:50.351869Z",
          "iopub.status.idle": "2022-12-26T05:07:57.004477Z",
          "shell.execute_reply.started": "2022-12-26T05:07:50.351827Z",
          "shell.execute_reply": "2022-12-26T05:07:57.003394Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One obvious question is why do we append the Valid and Test set to the X_filter. The model has not \"observed\" them during training. We do so because, we would like to evaluate a test triple against it's corruptions and not against known facts. If we know that the Validation triples and Test triples are facts (and not queries), we need to filter these triples out of the generated corruptions. This is the standard procedure that is used to compute the metrics to compete on the leadership board."
      ],
      "metadata": {
        "id": "CdAw7EUMlYon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Aggregate metrics\n",
        "\n",
        "\n",
        "Once we have the ranks for all the test set triples, we can compute the following aggregate metrics: **MR**, **MRR**, **Hits@N**. These metrics indicate the overall quality of the model on a test set. These metrics come from Information Retrieval domain and are always computed on a set of **True Statements**. To illustrate each of these metric let us first create a small test set of 5 triples and compute their ranks."
      ],
      "metadata": {
        "id": "W_YEKhYglae3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_small = np.array(\n",
        "                [['doctorate',\n",
        "                    '/education/educational_degree/people_with_this_degree./education/education/major_field_of_study',\n",
        "                    'computer engineering'],\n",
        "\n",
        "                ['star wars',\n",
        "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
        "                    'united states dollar'],\n",
        "\n",
        "                ['harry potter and the chamber of secrets',\n",
        "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
        "                    'united states dollar'],\n",
        "\n",
        "                ['star wars', '/film/film/language', 'english language'],\n",
        "                ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']])\n",
        "\n",
        "\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "ranks = evaluate_performance(X_test_small, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter, \n",
        "                             corrupt_side='s,o')\n",
        "print(ranks)"
      ],
      "metadata": {
        "id": "ZXe0FgPeC_Si",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:07:57.005848Z",
          "iopub.execute_input": "2022-12-26T05:07:57.006885Z",
          "iopub.status.idle": "2022-12-26T05:08:03.309312Z",
          "shell.execute_reply.started": "2022-12-26T05:07:57.006844Z",
          "shell.execute_reply": "2022-12-26T05:08:03.308221Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us look at each aggregate metrics in detail:\n",
        "\n",
        "- **Mean rank (MR)**, as the name indicates, is the average of all the ranks of the triples. The value ranges from 1 (ideal case when all ranks equal to 1) to number of corruptions (where all the ranks are last).\n",
        "\n",
        "![mr formula](https://user-images.githubusercontent.com/281477/90627586-105d6180-e214-11ea-84d4-c5d3e4b089f4.png)"
      ],
      "metadata": {
        "id": "KKy0b0XVlgRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mr_score\n",
        "print('MR :', mr_score(ranks))"
      ],
      "metadata": {
        "id": "7aaCDgkFldn6",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.310553Z",
          "iopub.execute_input": "2022-12-26T05:08:03.311133Z",
          "iopub.status.idle": "2022-12-26T05:08:03.318217Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.311089Z",
          "shell.execute_reply": "2022-12-26T05:08:03.317102Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Mean reciprocal rank (MRR)**, is the average of the reciprocal ranks of all the triples. The value ranges from 0 to 1; higher the value better is the model.\n",
        "\n",
        "![mrr formula](https://user-images.githubusercontent.com/281477/90627604-12272500-e214-11ea-9777-5d30b23f0d6f.png)"
      ],
      "metadata": {
        "id": "bDyhtqxYYMmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mrr_score\n",
        "print('MRR :', mrr_score(ranks))"
      ],
      "metadata": {
        "id": "8El8j03AYL5d",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.319672Z",
          "iopub.execute_input": "2022-12-26T05:08:03.320835Z",
          "iopub.status.idle": "2022-12-26T05:08:03.336284Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.320784Z",
          "shell.execute_reply": "2022-12-26T05:08:03.334565Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRR is an indicator of mean rank after removing the effect of outliers."
      ],
      "metadata": {
        "id": "Wcw8Sgc3lmPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean rank after removing the outlier effect: ', np.ceil(1/mrr_score(ranks)))"
      ],
      "metadata": {
        "id": "cbWTp59JlnaH",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.337555Z",
          "iopub.execute_input": "2022-12-26T05:08:03.338049Z",
          "iopub.status.idle": "2022-12-26T05:08:03.352919Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.338001Z",
          "shell.execute_reply": "2022-12-26T05:08:03.351475Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **hits@n** is the percentage of computed ranks that are greater than (in terms of ranking) or equal to a rank of n. The value ranges from 0 to 1; higher the value better is the model.\n",
        "\n",
        "![hits formula](https://user-images.githubusercontent.com/281477/90627565-09365380-e214-11ea-81c8-292a3de016d0.png)"
      ],
      "metadata": {
        "id": "3d_YGuEJlpxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import hits_at_n_score\n",
        "print('hits@1 :', hits_at_n_score(ranks, 1))\n",
        "print('hits@3 :', hits_at_n_score(ranks, 3))\n",
        "print('hits@5 :', hits_at_n_score(ranks, 5))\n",
        "print('hits@10 :', hits_at_n_score(ranks, 10))"
      ],
      "metadata": {
        "id": "u5YHZs9elojV",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.354513Z",
          "iopub.execute_input": "2022-12-26T05:08:03.355607Z",
          "iopub.status.idle": "2022-12-26T05:08:03.369086Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.355563Z",
          "shell.execute_reply": "2022-12-26T05:08:03.367948Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print unique entities\n",
        "print('Number of unique entities:', len(model.ent_to_idx))"
      ],
      "metadata": {
        "id": "IzvhHy6XIHXI",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.370497Z",
          "iopub.execute_input": "2022-12-26T05:08:03.371514Z",
          "iopub.status.idle": "2022-12-26T05:08:03.386979Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.371475Z",
          "shell.execute_reply": "2022-12-26T05:08:03.385293Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if, for a model, you observe that on a test set, the MRR score is 0.01? Is it a good model?**\n",
        "\n",
        "It is not very straightforward. What the above value means is that if you remove the outlier effect, on an average the ranks are around 100 (1/0.01). It may be a good/bad value. It depends on number of corruptions that you have used for the computation. Say you had 1 million corruptions and yet the mrr score was 0.01. The model, in general, was quite good at ranking against 1 million corruption because on an average it gave a rank of close to 100. But say if the corruptions were only 100 and we had an mrr of 0.01, it means that the model did a very bad task at ranking the test triples against just 100 corruptions.\n",
        "\n",
        "On a real life dataset, on should take a closer look at **hits@n** values and decide whether the model is a good model or not. ***The choice of n should depend on the number of corruptions that are being generated per test triple***. If a large percentage of ranks computed on the test set triple falls within the n ranks, then the model can be considered as a good model."
      ],
      "metadata": {
        "id": "EzUfheLiOfwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_aggregate_metrics(ranks):\n",
        "    print('Mean Rank:', mr_score(ranks)) \n",
        "    print('Mean Reciprocal Rank:', mrr_score(ranks)) \n",
        "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
        "    print('Hits@10:', hits_at_n_score(ranks, 10))\n",
        "    print('Hits@100:', hits_at_n_score(ranks, 100))\n",
        "\n",
        "\n",
        "display_aggregate_metrics(ranks)\n"
      ],
      "metadata": {
        "id": "GLxF6F1Blxf1",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.388650Z",
          "iopub.execute_input": "2022-12-26T05:08:03.389549Z",
          "iopub.status.idle": "2022-12-26T05:08:03.399745Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.389493Z",
          "shell.execute_reply": "2022-12-26T05:08:03.398670Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.4. Training with early stopping\n",
        "\n",
        "While training a model, we would like to make sure that the model does not overfit or under fit on the data. If we train a model for a fixed number of epochs, we will not know whether the model has underfit or overfit the training data. Hence it is necessary to test the model performance on a held out set at regular intervals to decide when to stop training. This is called \"Early stopping\", i.e. we don't let the model run for a long time but stop much before when the performance on the held out set starts to degrade. \n",
        "\n",
        "However we also do not want to model to overfit on the held out set and limit the generalization capabilities of the model. Hence we should create both a validation set and a test set to verify the generalization capability of the model, and to make sure that we dont over fit and under fit on the data.  "
      ],
      "metadata": {
        "id": "HM0ax6rPpbpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_params = { 'x_valid': X_valid,   # Validation set on which early stopping will be performed\n",
        "                          'criteria': 'mrr',    # metric to watch during early stopping\n",
        "                          'burn_in': 150,       # Burn in time, i.e. early stopping checks will not be performed till 150 epochs\n",
        "                          'check_interval': 50, # After burn in time, early stopping checks will be performed at every 50th epochs (i.e. 150, 200, 250, ...)\n",
        "                          'stop_interval': 2,   # If the monitored criteria degrades for these many epochs, the training stops. \n",
        "                          'corrupt_side':'s,o'  # Which sides to corrupt furing early stopping evaluation (default both subject and obj as described earlier)\n",
        "                        }\n",
        "\n",
        "# create a model as earlier\n",
        "model = TransE(k=100, \n",
        "               epochs=10000, \n",
        "               eta=1, \n",
        "               loss='multiclass_nll', \n",
        "               initializer='xavier', initializer_params={'uniform': False},\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "               seed= 0, batches_count= 1, verbose=True)\n",
        "\n",
        "# call model.fit by passing early stopping params\n",
        "model.fit(X_train,                                      # training set\n",
        "          early_stopping=True,                          # set early stopping to true\n",
        "          early_stopping_params=early_stopping_params)  # pass the early stopping params\n",
        "\n",
        "# evaluate the model with filter\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter)\n",
        "# display the metrics\n",
        "display_aggregate_metrics(ranks)"
      ],
      "metadata": {
        "id": "DagdzuwspU1Q",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:08:03.401323Z",
          "iopub.execute_input": "2022-12-26T05:08:03.401698Z",
          "iopub.status.idle": "2022-12-26T05:20:32.268470Z",
          "shell.execute_reply.started": "2022-12-26T05:08:03.401664Z",
          "shell.execute_reply": "2022-12-26T05:20:32.267097Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary so far\n"
      ],
      "metadata": {
        "id": "Ic7r20ScpS78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Generate train/test data\n",
        "# create train/test/valid splits, train the model and evaluate using train_test_split_no_unseen API\n",
        "from ampligraph.evaluation import train_test_split_no_unseen\n",
        "# get the validation set of size 500\n",
        "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed=0)\n",
        "\n",
        "# get the test set of size 1000 from the remaining triples\n",
        "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed=0)\n",
        "# ----------------------\n",
        "# Training:\n",
        "\n",
        "print('Training set:', X_train.shape)\n",
        "\n",
        "# Train a KGE model\n",
        "model = TransE(k=300, \n",
        "               epochs=100, \n",
        "               eta=1, \n",
        "               loss='multiclass_nll', \n",
        "               initializer='xavier', initializer_params={'uniform': False},\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.0001}, \n",
        "               seed= 0, batches_count= 10, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "# ----------------------\n",
        "# Evaluate: \n",
        "# Filtered evaluation with ranking strategy assigning worst rank to break ties\n",
        "\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "save_model(model, 'TransE.pkl')\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "# create the filter \n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "# compute ranks\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "# ranks are computed per triple\n",
        "print('Test set:', X_test.shape)\n",
        "print('Size of ranks:', ranks.shape)\n",
        "\n",
        "# Aggregate metrics show the aggregate performance of the model on the test set using a single number\n",
        "display_aggregate_metrics(ranks)\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "hBXrmiA1lzjO",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:20:32.274267Z",
          "iopub.execute_input": "2022-12-26T05:20:32.274689Z",
          "iopub.status.idle": "2022-12-26T05:24:22.007677Z",
          "shell.execute_reply.started": "2022-12-26T05:20:32.274655Z",
          "shell.execute_reply": "2022-12-26T05:24:22.006118Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Takeaways**\n",
        "- `train_test_split_no_unseen` API can be used to generate train/test splits such that test set contains only entities 'seen' during training\n",
        "- Once a model is trained, one can use `model.predict` to choose from a set of hypothesis based on the scores returned by the model.\n",
        "- One can access the quality of model on a **test set of True Facts** by using metrics such as MR, MRR and hits@n\n",
        "- We can use early stopping to prevent model from over/under fitting by using a Validation Set."
      ],
      "metadata": {
        "id": "gP2K-mHgQ54K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Knowledge Discovery \n",
        "\n",
        "In Ampligraph we provide a number of high-level convenience functions for performing knowledge discovery using graph embeddings:\n",
        "\n",
        "> ***query_topn***: which when given two elements of a triple will return the top_n results of all possible completions ordered by predicted score.\n",
        "\n",
        "> ***discover_facts***: generate a set of candidate statements using one of several defined strategies and return triples that perform well when evaluated against corruptions.\n",
        "\n",
        "> ***find_clusters***: perform link-based cluster analysis on graph embeddings.\n",
        "\n",
        "> ***find_duplicates***: which will find duplicate entities in a graph based on their embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "pdX1lwK4rX_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Triple completion\n",
        "\n",
        "Sometimes you may have either a relation and entity (head or tail) pair, or just two entities, and you want to see what the top n results returned by the model are that completes the triple. \n",
        "\n",
        "``` \n",
        "    <head, relation, ?> \n",
        "    <head, ?,        tail>\n",
        "    <?,    relation, tail>\n",
        "```\n",
        "\n",
        "Specify ```rels_to_consider``` or ```ents_to_consider``` lists to return triples where the missing element is filled only from that list. \n"
      ],
      "metadata": {
        "id": "N3dmkTgHrdB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.discovery import query_topn\n",
        "\n",
        "# restore the previously saved model to save time\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "triples, scores = query_topn(model, top_n=5, \n",
        "                             head='missy elliott', \n",
        "                             relation='/people/person/profession', \n",
        "                             tail=None, \n",
        "                             ents_to_consider=None, \n",
        "                             rels_to_consider=None)\n",
        "\n",
        "for triple, score in zip(triples, scores):\n",
        "    print('Score: {} \\t {} '.format(score, triple))"
      ],
      "metadata": {
        "id": "7U1Wur_hravz",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:24:22.009545Z",
          "iopub.execute_input": "2022-12-26T05:24:22.009991Z",
          "iopub.status.idle": "2022-12-26T05:24:22.280374Z",
          "shell.execute_reply.started": "2022-12-26T05:24:22.009946Z",
          "shell.execute_reply": "2022-12-26T05:24:22.278687Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triples, scores = query_topn(model, top_n=10, \n",
        "                             head='the departed', \n",
        "                             relation=None, \n",
        "                             tail='/m/086k8', \n",
        "                             ents_to_consider=None, \n",
        "                             rels_to_consider=None)\n",
        "\n",
        "for triple, score in zip(triples, scores):\n",
        "    print('Score: {} \\t {} '.format(score, triple))"
      ],
      "metadata": {
        "id": "6i8J-rFvIPuB",
        "execution": {
          "iopub.status.busy": "2022-12-26T05:24:22.282076Z",
          "iopub.execute_input": "2022-12-26T05:24:22.282605Z",
          "iopub.status.idle": "2022-12-26T05:24:22.467714Z",
          "shell.execute_reply.started": "2022-12-26T05:24:22.282557Z",
          "shell.execute_reply": "2022-12-26T05:24:22.466336Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.discovery import query_topn\n",
        "\n",
        "# restore the previously saved model to save time\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "triples, scores = query_topn(model, top_n=5, \n",
        "                             head=None, \n",
        "                             relation='/people/person/profession', \n",
        "                             tail='musician', \n",
        "                             ents_to_consider=None, \n",
        "                             rels_to_consider=None)\n",
        "\n",
        "for triple, score in zip(triples, scores):\n",
        "    print('Score: {} \\t {} '.format(score, triple))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-26T05:24:22.469612Z",
          "iopub.execute_input": "2022-12-26T05:24:22.470010Z",
          "iopub.status.idle": "2022-12-26T05:24:22.727666Z",
          "shell.execute_reply.started": "2022-12-26T05:24:22.469972Z",
          "shell.execute_reply": "2022-12-26T05:24:22.726293Z"
        },
        "trusted": true,
        "id": "RL06KlTVZBMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
